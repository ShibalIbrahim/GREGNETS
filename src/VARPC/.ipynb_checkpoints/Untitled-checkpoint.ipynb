{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "26d4f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"\"\"\n",
    "/home/gridsan/shibal/.conda/envs/GraphNetworks/bin/python -u /home/gridsan/shibal/GREGNETS/src/VARPC/VARPC_tuning.py \\\n",
    "--load_directory /home/gridsan/shibal/FinancialForecasting_shared/data/ --cohort 'SP1500' --time_series 'volatilities' \\\n",
    "--regularizer 'Lasso' --KG_mask 'soft' --mask_sparsity 100  \\\n",
    "--num_training_years 2 --n_steps 1 \\\n",
    "--version 2 |& tee -a '/home/gridsan/shibal/GREGNETS/src/VARPC/logs-SP1500/MODELVARPC_COHORTSP1500_TSvolume_REGLasso_KGcosearch_MASKsoft_LEVEL100_Y2_NSTEPS1_V2/output.txt'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5dd69c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tee: /home/gridsan/shibal/GREGNETS/src/VARPC/logs-SP1500/MODELVARPC_COHORTSP1500_TSvolume_REGLasso_KGcosearch_MASKsoft_LEVEL100_Y2_NSTEPS1_V2/output.txt: No such file or directory\n",
      "3.7.10 (default, Feb 26 2021, 18:47:35) \n",
      "[GCC 7.3.0] linux /home/gridsan/shibal/.conda/envs/GraphNetworks/bin/python\n",
      "/home/gridsan/shibal/FinancialForecasting_shared/data/\n",
      "volatilities/SP1500/yahoo_companies_volatilities_residuals_2Y.csv\n",
      "(N, T, p): (503, 1072, 1)\n",
      "Training Data Shapes: (503, 1072, 1) , (503, 1072)\n",
      "Validation Data Shapes: (252, 1072, 1) , (252, 1072)\n",
      "Test Data Shapes: (252, 1072, 1) , (252, 1072)\n",
      "Starting optimization for pre-estimator for A...\n",
      "Iter    objective   relative_objective\n",
      "    0    0.116599    inf\n",
      "  100    0.088848    0.000333\n",
      "  200    0.087811    0.000026\n",
      "Solution converged in 240 iterations with final objective=0.08775 and relative objective=0.00000978<=1e-05\n",
      "/home/gridsan/shibal/GREGNETS/src/VARPC/pre_estimators.py:285: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  M[i,j] = cmax/co[i,j]\n",
      "/home/gridsan/shibal/GREGNETS/src/VARPC/pre_estimators.py:286: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  M[j,i] = cmax/co[i,j]\n",
      "lambda_A:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "lambda_rho:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Warm start either not set or it is yet to warm up.\n",
      "Initializing using arguments from init or as a default zero matrix\n",
      "Starting optimization for lambda_A: 0.0001 lambda_rho: 0.0001\n",
      "Iter    objective   convergence_count   no_progress_count\n",
      "    0    0.174357773    0   0\n",
      "   10    0.172149855    0   0\n",
      "   20    0.170987465    0   0\n",
      "   30    0.170279613    0   0\n",
      "   40    0.169792619    0   0\n",
      "   50    0.169422195    0   0\n",
      "   60    0.169121057    0   0\n",
      "   70    0.168867181    0   0\n",
      "   80    0.168645726    0   0\n",
      "   90    0.168449876    0   0\n",
      "  100    0.168275158    0   0\n",
      "  110    0.168117286    0   0\n",
      "  120    0.167973317    0   0\n",
      "  130    0.167841117    0   0\n",
      "  140    0.167720112    0   0\n",
      "  150    0.167609320    0   0\n",
      "  160    0.167508419    0   0\n",
      "  170    0.167415314    0   0\n",
      "  180    0.167328813    0   0\n",
      "  190    0.167248046    0   0\n",
      "  200    0.167172753    0   0\n",
      "  210    0.167102265    0   0\n",
      "  220    0.167036293    0   0\n",
      "  230    0.166974759    0   0\n",
      "  240    0.166917087    0   0\n",
      "  250    0.166863095    0   0\n",
      "  260    0.166812225    0   0\n",
      "  270    0.166764125    0   0\n",
      "  280    0.166718507    0   0\n",
      "  290    0.166675210    0   0\n",
      "  300    0.166634489    0   0\n",
      "  310    0.166596014    0   0\n",
      "  320    0.166559457    0   0\n",
      "  330    0.166524777    0   0\n",
      "  340    0.166491933    0   0\n",
      "  350    0.166460882    0   0\n",
      "  360    0.166431434    0   0\n",
      "  370    0.166403264    0   0\n",
      "  380    0.166376447    0   0\n",
      "  390    0.166351072    0   0\n",
      "  400    0.166326793    0   0\n",
      "  410    0.166303577    0   0\n",
      "  420    0.166281345    0   0\n",
      "  430    0.166260134    0   0\n",
      "  440    0.166240067    0   0\n",
      "  450    0.166221095    0   0\n",
      "  460    0.166203111    0   0\n",
      "  470    0.166185899    0   0\n",
      "  480    0.166169377    0   0\n",
      "  490    0.166153569    0   0\n",
      "  500    0.166138436    0   0\n",
      "  510    0.166123936    0   0\n",
      "  520    0.166110064    0   0\n",
      "  530    0.166096780    0   0\n",
      "  540    0.166083977    0   0\n",
      "  550    0.166071627    0   0\n",
      "  560    0.166059708    0   0\n",
      "  570    0.166048233    0   0\n",
      "  580    0.166037167    0   0\n",
      "  590    0.166026559    0   0\n",
      "  600    0.166016317    0   0\n",
      "  610    0.166006420    8   0\n",
      "Solution converged in 612 iterations with final\n",
      "                                            objective=0.166004479, slow progress with global relative\n",
      "                                            objective=0.000000969<=1e-06 for 10 non-consecutive\n",
      "                                            iterations\n",
      "  612    0.166004479    10   0\n",
      "0.000100000000  0.000100000000  0.15481  0.15278  0.06306  0.05\n",
      "Starting optimization for lambda_A: 0.0001 lambda_rho: 7.498942093324559e-05\n",
      "Iter    objective   convergence_count   no_progress_count\n",
      "    0    0.163203178    0   0\n",
      "   10    0.163075618    0   0\n",
      "   20    0.162970248    0   0\n",
      "   30    0.162880713    0   0\n",
      "   40    0.162803205    0   0\n",
      "   50    0.162735271    0   0\n",
      "   60    0.162675161    0   0\n",
      "   70    0.162621562    0   0\n",
      "   80    0.162573477    0   0\n",
      "   90    0.162530090    0   0\n",
      "  100    0.162490724    0   0\n",
      "  110    0.162454808    0   0\n",
      "  120    0.162421884    0   0\n",
      "  130    0.162391617    0   0\n",
      "  140    0.162363719    0   0\n",
      "  150    0.162337951    0   0\n",
      "  160    0.162314014    0   0\n",
      "  170    0.162291695    0   0\n",
      "  180    0.162270884    0   0\n",
      "  190    0.162251436    0   0\n",
      "  200    0.162233209    0   0\n",
      "  210    0.162216088    0   0\n",
      "  220    0.162199965    0   0\n",
      "  230    0.162184730    0   0\n",
      "  240    0.162170324    0   0\n",
      "  250    0.162156692    0   0\n",
      "  260    0.162143812    0   0\n",
      "  270    0.162131616    0   0\n",
      "  280    0.162120052    0   0\n",
      "  290    0.162109123    0   0\n",
      "  300    0.162098719    0   0\n",
      "  310    0.162088796    7   0\n",
      "Solution converged in 313 iterations with final\n",
      "                                            objective=0.162085909, slow progress with global relative\n",
      "                                            objective=0.000000958<=1e-06 for 10 non-consecutive\n",
      "                                            iterations\n",
      "  313    0.162085909    10   0\n",
      "0.000100000000  0.000074989421  0.14876  0.14799  0.08496  0.08\n",
      "Starting optimization for lambda_A: 0.0001 lambda_rho: 5.623413251903491e-05\n",
      "Iter    objective   convergence_count   no_progress_count\n",
      "    0    0.158751188    0   0\n",
      "   10    0.158602050    0   0\n",
      "   20    0.158479174    0   0\n",
      "   30    0.158375081    0   0\n",
      "   40    0.158285203    0   0\n",
      "   50    0.158206544    0   0\n",
      "   60    0.158136983    0   0\n",
      "   70    0.158074930    0   0\n",
      "   80    0.158019143    0   0\n",
      "   90    0.157968718    0   0\n",
      "  100    0.157922838    0   0\n",
      "  110    0.157880894    0   0\n",
      "  120    0.157842369    0   0\n",
      "  130    0.157806889    0   0\n",
      "  140    0.157774062    0   0\n",
      "  150    0.157743647    0   0\n",
      "  160    0.157715318    0   0\n",
      "  170    0.157688880    0   0\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "47038320",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"\"\"\n",
    "/home/gridsan/shibal/.conda/envs/GraphNetworks/bin/python -u /home/gridsan/shibal/GREGNETS/src/NNPC/NGCNPC-Tuning.py \\\n",
    "--load_directory '/home/gridsan/shibal/FinancialForecasting_shared/data/' --time_series 'volatilities' --cohort 'SP1500' \\\n",
    "--num_training_years 2 \\\n",
    "--no-mask \\\n",
    "--ntrials 2 --version 1 --tuning_seed 0 \\\n",
    "|& tee -a '/home/gridsan/shibal/GREGNETS/src/NNPC/logs-SP1500/MODELNGCNPC_COHORTSP1500_TSvolatilities_REGLasso_KGcosearch_MASKTrue_Y2_V1/output.txt'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "35e49273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.10 (default, Feb 26 2021, 18:47:35) \n",
      "[GCC 7.3.0] linux /home/gridsan/shibal/.conda/envs/GraphNetworks/bin/python\n",
      "2022-10-26 07:48:53.107132: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-10-26 07:48:56.072169: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-10-26 07:48:56.072396: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /state/partition1/llgrid/pkg/anaconda/anaconda3-2021a/pkgs/cudatoolkit-10.2.89-hfd86e86_1/lib:/state/partition1/llgrid/pkg/anaconda/anaconda3-2021a/lib\n",
      "2022-10-26 07:48:56.072409: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-26 07:48:56.072431: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (d-6-4-2): /proc/driver/nvidia/version does not exist\n",
      "2022-10-26 07:48:56.072706: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-26 07:48:56.082942: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "/home/gridsan/shibal/GREGNETS/src/data_utils.py:337: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  start_date=pd.datetime(2005,1,1),\n",
      "/home/gridsan/shibal/GREGNETS/src/data_utils.py:338: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  end_date=pd.datetime(2020,12,31)\n",
      "/home/gridsan/shibal/GREGNETS/src/data_utils.py:895: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  start_date=pd.datetime(2005,1,1),\n",
      "/home/gridsan/shibal/GREGNETS/src/data_utils.py:896: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  end_date=pd.datetime(2020,12,31)):\n",
      "/home/gridsan/shibal/FinancialForecasting_shared/data/\n",
      "volatilities/SP1500/yahoo_companies_volatilities_residuals_2Y.csv\n",
      "(440, 64, 1072) (440, 1072) (252, 64, 1072) (252, 1072) (252, 64, 1072) (252, 1072)\n",
      "(440, 1072, 64) (440, 1072) (252, 1072, 64) (252, 1072) (252, 1072, 64) (252, 1072)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method NGraphConvolution.call of <src.NNPC.Layers.layers.NGraphConvolution object at 0x7ff4046887d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 1072, 64)]        0         \n",
      "_________________________________________________________________\n",
      "n_graph_convolution (NGraphC (None, 1072, 1)           2113      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1072, 1)           0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 1072)              0         \n",
      "=================================================================\n",
      "Total params: 2,113\n",
      "Trainable params: 2,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-26 07:49:06.559456: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-10-26 07:49:07.943982: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2400000000 Hz\n",
      "Epoch 1/10000\n",
      "7/7 [==============================] - 6s 171ms/step - loss: 0.3520 - mse: 0.3520 - val_loss: 0.2864 - val_mse: 0.2864\n",
      "Epoch 2/10000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.2822 - mse: 0.2822 - val_loss: 0.2306 - val_mse: 0.2306\n",
      "Epoch 3/10000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.2313 - mse: 0.2313 - val_loss: 0.1956 - val_mse: 0.1956\n",
      "Epoch 4/10000\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.1970 - mse: 0.1970 - val_loss: 0.1696 - val_mse: 0.1696\n",
      "Epoch 5/10000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1725 - mse: 0.1725 - val_loss: 0.1527 - val_mse: 0.1527\n",
      "Epoch 6/10000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1563 - mse: 0.1563 - val_loss: 0.1408 - val_mse: 0.1408\n",
      "Epoch 7/10000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1427 - mse: 0.1427 - val_loss: 0.1314 - val_mse: 0.1314\n",
      "Epoch 8/10000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1329 - mse: 0.1329 - val_loss: 0.1239 - val_mse: 0.1239\n",
      "Epoch 9/10000\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.1252 - mse: 0.1252 - val_loss: 0.1181 - val_mse: 0.1181\n",
      "Epoch 10/10000\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 0.1196 - mse: 0.1196 - val_loss: 0.1137 - val_mse: 0.1137\n",
      "Epoch 11/10000\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.1165 - mse: 0.1165 - val_loss: 0.1103 - val_mse: 0.1103\n",
      "Epoch 12/10000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1117 - mse: 0.1117 - val_loss: 0.1077 - val_mse: 0.1077\n",
      "Epoch 13/10000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.1096 - mse: 0.1096 - val_loss: 0.1058 - val_mse: 0.1058\n",
      "Epoch 14/10000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.1084 - mse: 0.1084 - val_loss: 0.1042 - val_mse: 0.1042\n",
      "Epoch 15/10000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1064 - mse: 0.1064 - val_loss: 0.1030 - val_mse: 0.1030\n",
      "Epoch 16/10000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1057 - mse: 0.1057 - val_loss: 0.1021 - val_mse: 0.1021\n",
      "Epoch 17/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1055 - mse: 0.1055 - val_loss: 0.1013 - val_mse: 0.1013\n",
      "Epoch 18/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1039 - mse: 0.1039 - val_loss: 0.1007 - val_mse: 0.1007\n",
      "Epoch 19/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1031 - mse: 0.1031 - val_loss: 0.1002 - val_mse: 0.1002\n",
      "Epoch 20/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1026 - mse: 0.1026 - val_loss: 0.0998 - val_mse: 0.0998\n",
      "Epoch 21/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1022 - mse: 0.1022 - val_loss: 0.0994 - val_mse: 0.0994\n",
      "Epoch 22/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1036 - mse: 0.1036 - val_loss: 0.0990 - val_mse: 0.0990\n",
      "Epoch 23/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1019 - mse: 0.1019 - val_loss: 0.0988 - val_mse: 0.0988\n",
      "Epoch 24/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1016 - mse: 0.1016 - val_loss: 0.0985 - val_mse: 0.0985\n",
      "Epoch 25/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1009 - mse: 0.1009 - val_loss: 0.0983 - val_mse: 0.0983\n",
      "Epoch 26/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1008 - mse: 0.1008 - val_loss: 0.0981 - val_mse: 0.0981\n",
      "Epoch 27/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1012 - mse: 0.1012 - val_loss: 0.0980 - val_mse: 0.0980\n",
      "Epoch 28/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1015 - mse: 0.1015 - val_loss: 0.0978 - val_mse: 0.0978\n",
      "Epoch 29/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1002 - mse: 0.1002 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 30/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1012 - mse: 0.1012 - val_loss: 0.0975 - val_mse: 0.0975\n",
      "Epoch 31/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1009 - mse: 0.1009 - val_loss: 0.0974 - val_mse: 0.0974\n",
      "Epoch 32/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1000 - mse: 0.1000 - val_loss: 0.0973 - val_mse: 0.0973\n",
      "Epoch 33/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0994 - mse: 0.0994 - val_loss: 0.0972 - val_mse: 0.0972\n",
      "Epoch 34/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0998 - mse: 0.0998 - val_loss: 0.0970 - val_mse: 0.0970\n",
      "Epoch 35/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0997 - mse: 0.0997 - val_loss: 0.0970 - val_mse: 0.0970\n",
      "Epoch 36/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0990 - mse: 0.0990 - val_loss: 0.0969 - val_mse: 0.0969\n",
      "Epoch 37/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.0968 - val_mse: 0.0968\n",
      "Epoch 38/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1001 - mse: 0.1001 - val_loss: 0.0967 - val_mse: 0.0967\n",
      "Epoch 39/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0990 - mse: 0.0990 - val_loss: 0.0966 - val_mse: 0.0966\n",
      "Epoch 40/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0991 - mse: 0.0991 - val_loss: 0.0966 - val_mse: 0.0966\n",
      "Epoch 41/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.0965 - val_mse: 0.0965\n",
      "Epoch 42/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1008 - mse: 0.1008 - val_loss: 0.0965 - val_mse: 0.0965\n",
      "Epoch 43/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0964 - val_mse: 0.0964\n",
      "Epoch 44/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.1004 - mse: 0.1004 - val_loss: 0.0963 - val_mse: 0.0963\n",
      "Epoch 45/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0995 - mse: 0.0995 - val_loss: 0.0963 - val_mse: 0.0963\n",
      "Epoch 46/10000\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 0.0998 - mse: 0.0998 - val_loss: 0.0963 - val_mse: 0.0963\n",
      "Epoch 47/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0962 - val_mse: 0.0962\n",
      "Epoch 48/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0996 - mse: 0.0996 - val_loss: 0.0961 - val_mse: 0.0961\n",
      "Epoch 49/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0981 - mse: 0.0981 - val_loss: 0.0961 - val_mse: 0.0961\n",
      "Epoch 50/10000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.0961 - val_mse: 0.0961\n",
      "Epoch 51/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0998 - mse: 0.0998 - val_loss: 0.0961 - val_mse: 0.0961\n",
      "Epoch 52/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.0960 - val_mse: 0.0960\n",
      "Epoch 53/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0960 - val_mse: 0.0960\n",
      "Epoch 54/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.0961 - val_mse: 0.0961\n",
      "Epoch 55/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0959 - val_mse: 0.0959\n",
      "Epoch 56/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0959 - val_mse: 0.0959\n",
      "Epoch 57/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.0959 - val_mse: 0.0959\n",
      "Epoch 58/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0959 - val_mse: 0.0959\n",
      "Epoch 59/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 60/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1009 - mse: 0.1009 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 61/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0959 - val_mse: 0.0959\n",
      "Epoch 62/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 63/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0971 - mse: 0.0971 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 64/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 65/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 66/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 67/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0994 - mse: 0.0994 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 68/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 69/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 70/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 71/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 72/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 73/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0990 - mse: 0.0990 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 74/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 75/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 76/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 77/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 78/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 79/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 80/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 81/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0991 - mse: 0.0991 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 82/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1003 - mse: 0.1003 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 83/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0995 - mse: 0.0995 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 84/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0996 - mse: 0.0996 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 85/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 86/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 87/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 88/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0972 - mse: 0.0972 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 89/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0975 - mse: 0.0975 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 90/10000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0973 - mse: 0.097 - 0s 24ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 91/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 92/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0970 - mse: 0.0970 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 93/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 94/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 95/10000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 96/10000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0994 - mse: 0.0994 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 97/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 98/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 99/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 100/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 101/10000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 102/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 103/10000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 104/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 105/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0990 - mse: 0.0990 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 106/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 107/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 108/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 109/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 110/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0980 - mse: 0.0980 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 111/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0972 - mse: 0.0972 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 112/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 113/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 114/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 115/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0990 - mse: 0.0990 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 116/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 117/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0980 - mse: 0.0980 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 118/10000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 119/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0981 - mse: 0.0981 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 120/10000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 121/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 122/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 123/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0975 - mse: 0.0975 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 124/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 125/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 126/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0975 - mse: 0.0975 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 127/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 128/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0995 - mse: 0.0995 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 129/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 130/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 131/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 132/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 133/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0972 - mse: 0.0972 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 134/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 135/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 136/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 137/10000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0975 - mse: 0.097 - 0s 24ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 138/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 139/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0980 - mse: 0.0980 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 140/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 141/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0968 - mse: 0.0968 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 142/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 143/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 144/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 145/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 146/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0981 - mse: 0.0981 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 147/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0981 - mse: 0.0981 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 148/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0991 - mse: 0.0991 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 149/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 150/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0965 - mse: 0.0965 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 151/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 152/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0999 - mse: 0.0999 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 153/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.0954 - val_mse: 0.0954\n",
      "Epoch 154/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 155/10000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 156/10000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 157/10000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0954 - val_mse: 0.0954\n",
      "Epoch 158/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 159/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0997 - mse: 0.0997 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 160/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 161/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 162/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 163/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0966 - mse: 0.0966 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 164/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 165/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 166/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 167/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 168/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 169/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 170/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 171/10000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0970 - mse: 0.0970 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 172/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 173/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 174/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 175/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 176/10000\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 177/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Params testing:                                                                 \n",
      "{'K': 1, 'activation': 'relu', 'batch_size': 32, 'dropout': 1e-07, 'epochs': 10000, 'kernel_regularizer': 1.274274985703132e-05, 'knowledge_graph_mask_rho': None, 'knowledge_graph_mask_sparsity_rho': 100, 'lambda_rho': 8.858667904100833e-05, 'latent_units': 8, 'lr': 0.0001623776739188721, 'n_steps': 60, 'num_layers': 1, 'optimizer': 'adam', 'output_activation': 'linear', 'post_learning_rate_rho': 1.0, 'regularizer_rho': 'Lasso', 'units': 8, 'use_bias': False}\n",
      "  0%|                                   | 0/500 [00:00<?, ?trial/s, best loss=?]WARNING:tensorflow:AutoGraph could not transform <bound method PseudoLikelihood.call of <src.NNPC.Layers.layers.PseudoLikelihood object at 0x7ff3fc102e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2022-10-26 07:49:49.245717: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_16\n",
      "2022-10-26 07:49:50.271095: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "\\lambda-rho:0.00008859, Iteration:0, J:0.10151, |\\Delta J /J|:inf, sparsity:8.80\n",
      "\\lambda-rho:0.00008859, Iteration:4, J:0.09873, |\\Delta J /J|:0.00007429, sparsity:0.96\n",
      "  0%|                                   | 0/500 [01:56<?, ?trial/s, best loss=?]2022-10-26 07:51:44.261104: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "MSE:0.09844, R^2:0.34397, MSE-val:0.09538, R^2-val:0.33550, sparsity:0.96       \n",
      "Params testing:                                                                 \n",
      "{'K': 2, 'activation': 'relu', 'batch_size': 32, 'dropout': 4.641588833612772e-05, 'epochs': 10000, 'kernel_regularizer': 3.359818286283781e-05, 'knowledge_graph_mask_rho': None, 'knowledge_graph_mask_sparsity_rho': 150, 'lambda_rho': 1e-07, 'latent_units': 16, 'lr': 0.000545559478116852, 'n_steps': 40, 'num_layers': 1, 'optimizer': 'adam', 'output_activation': 'linear', 'post_learning_rate_rho': 1.0, 'regularizer_rho': 'Lasso', 'units': 32, 'use_bias': False}\n",
      "  0%|    | 1/500 [01:58<16:24:36, 118.39s/trial, best loss: 0.09538302289975643]2022-10-26 07:51:47.182591: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_16\n",
      "2022-10-26 07:51:47.726026: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "\\lambda-rho:0.00000010, Iteration:0, J:0.04335, |\\Delta J /J|:inf, sparsity:99.91\n",
      "\\lambda-rho:0.00000010, Iteration:100, J:0.03734, |\\Delta J /J|:0.00117911, sparsity:99.91\n",
      "\\lambda-rho:0.00000010, Iteration:200, J:0.03348, |\\Delta J /J|:0.00101855, sparsity:99.90\n",
      "\\lambda-rho:0.00000010, Iteration:300, J:0.03042, |\\Delta J /J|:0.00091083, sparsity:99.90\n",
      "\\lambda-rho:0.00000010, Iteration:400, J:0.02789, |\\Delta J /J|:0.00082777, sparsity:99.90\n",
      "\\lambda-rho:0.00000010, Iteration:500, J:0.02577, |\\Delta J /J|:0.00076051, sparsity:99.90\n",
      "\\lambda-rho:0.00000010, Iteration:600, J:0.02395, |\\Delta J /J|:0.00070446, sparsity:99.90\n",
      "\\lambda-rho:0.00000010, Iteration:700, J:0.02238, |\\Delta J /J|:0.00065681, sparsity:99.90\n",
      "\\lambda-rho:0.00000010, Iteration:800, J:0.02100, |\\Delta J /J|:0.00061563, sparsity:99.90\n",
      "\\lambda-rho:0.00000010, Iteration:900, J:0.01978, |\\Delta J /J|:0.00057962, sparsity:99.90\n",
      "\\lambda-rho:0.00000010, Iteration:1000, J:0.01870, |\\Delta J /J|:0.00054779, sparsity:99.90\n",
      "\\lambda-rho:0.00000010, Iteration:1100, J:0.01773, |\\Delta J /J|:0.00051939, sparsity:99.89\n",
      "\\lambda-rho:0.00000010, Iteration:1200, J:0.01685, |\\Delta J /J|:0.00049387, sparsity:99.90\n",
      "\\lambda-rho:0.00000010, Iteration:1300, J:0.01606, |\\Delta J /J|:0.00047076, sparsity:99.90\n",
      "\\lambda-rho:0.00000010, Iteration:1400, J:0.01534, |\\Delta J /J|:0.00044972, sparsity:99.90\n",
      "\\lambda-rho:0.00000010, Iteration:1500, J:0.01468, |\\Delta J /J|:0.00043046, sparsity:99.89\n",
      "\\lambda-rho:0.00000010, Iteration:1600, J:0.01407, |\\Delta J /J|:0.00041274, sparsity:99.89\n",
      "\\lambda-rho:0.00000010, Iteration:1700, J:0.01351, |\\Delta J /J|:0.00039637, sparsity:99.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\lambda-rho:0.00000010, Iteration:1800, J:0.01300, |\\Delta J /J|:0.00038120, sparsity:99.89\n",
      "\\lambda-rho:0.00000010, Iteration:1900, J:0.01252, |\\Delta J /J|:0.00036708, sparsity:99.89\n",
      "\\lambda-rho:0.00000010, Iteration:2000, J:0.01208, |\\Delta J /J|:0.00035389, sparsity:99.89\n",
      "\\lambda-rho:0.00000010, Iteration:2100, J:0.01167, |\\Delta J /J|:0.00034154, sparsity:99.89\n",
      "\\lambda-rho:0.00000010, Iteration:2200, J:0.01128, |\\Delta J /J|:0.00032994, sparsity:99.89\n",
      "\\lambda-rho:0.00000010, Iteration:2300, J:0.01092, |\\Delta J /J|:0.00031903, sparsity:99.89\n",
      "\\lambda-rho:0.00000010, Iteration:2400, J:0.01058, |\\Delta J /J|:0.00030874, sparsity:99.89\n",
      "\\lambda-rho:0.00000010, Iteration:2500, J:0.01027, |\\Delta J /J|:0.00029900, sparsity:99.89\n",
      "\\lambda-rho:0.00000010, Iteration:2600, J:0.00997, |\\Delta J /J|:0.00028978, sparsity:99.88\n",
      "\\lambda-rho:0.00000010, Iteration:2700, J:0.00969, |\\Delta J /J|:0.00028102, sparsity:99.88\n",
      "\\lambda-rho:0.00000010, Iteration:2800, J:0.00943, |\\Delta J /J|:0.00027270, sparsity:99.88\n",
      "\\lambda-rho:0.00000010, Iteration:2900, J:0.00918, |\\Delta J /J|:0.00026477, sparsity:99.88\n",
      "\\lambda-rho:0.00000010, Iteration:3000, J:0.00894, |\\Delta J /J|:0.00025722, sparsity:99.88\n",
      "\\lambda-rho:0.00000010, Iteration:3100, J:0.00872, |\\Delta J /J|:0.00025001, sparsity:99.88\n",
      "\\lambda-rho:0.00000010, Iteration:3200, J:0.00850, |\\Delta J /J|:0.00024310, sparsity:99.88\n",
      "\\lambda-rho:0.00000010, Iteration:3300, J:0.00830, |\\Delta J /J|:0.00023649, sparsity:99.87\n",
      "\\lambda-rho:0.00000010, Iteration:3400, J:0.00811, |\\Delta J /J|:0.00023016, sparsity:99.87\n",
      "\\lambda-rho:0.00000010, Iteration:3500, J:0.00793, |\\Delta J /J|:0.00022408, sparsity:99.87\n",
      "\\lambda-rho:0.00000010, Iteration:3600, J:0.00776, |\\Delta J /J|:0.00021824, sparsity:99.87\n",
      "\\lambda-rho:0.00000010, Iteration:3700, J:0.00759, |\\Delta J /J|:0.00021263, sparsity:99.87\n",
      "\\lambda-rho:0.00000010, Iteration:3800, J:0.00743, |\\Delta J /J|:0.00020722, sparsity:99.87\n",
      "\\lambda-rho:0.00000010, Iteration:3900, J:0.00728, |\\Delta J /J|:0.00020201, sparsity:99.87\n",
      "\\lambda-rho:0.00000010, Iteration:4000, J:0.00714, |\\Delta J /J|:0.00019699, sparsity:99.87\n",
      "\\lambda-rho:0.00000010, Iteration:4100, J:0.00700, |\\Delta J /J|:0.00019215, sparsity:99.86\n",
      "\\lambda-rho:0.00000010, Iteration:4200, J:0.00687, |\\Delta J /J|:0.00018747, sparsity:99.86\n",
      "\\lambda-rho:0.00000010, Iteration:4300, J:0.00674, |\\Delta J /J|:0.00018295, sparsity:99.86\n",
      "\\lambda-rho:0.00000010, Iteration:4400, J:0.00662, |\\Delta J /J|:0.00017858, sparsity:99.86\n",
      "\\lambda-rho:0.00000010, Iteration:4500, J:0.00651, |\\Delta J /J|:0.00017436, sparsity:99.86\n",
      "\\lambda-rho:0.00000010, Iteration:4600, J:0.00640, |\\Delta J /J|:0.00017027, sparsity:99.86\n",
      "\\lambda-rho:0.00000010, Iteration:4700, J:0.00629, |\\Delta J /J|:0.00016631, sparsity:99.86\n",
      "\\lambda-rho:0.00000010, Iteration:4800, J:0.00619, |\\Delta J /J|:0.00016248, sparsity:99.85\n",
      "\\lambda-rho:0.00000010, Iteration:4900, J:0.00609, |\\Delta J /J|:0.00015877, sparsity:99.85\n",
      "\\lambda-rho:0.00000010, Iteration:5000, J:0.00599, |\\Delta J /J|:0.00015517, sparsity:99.85\n",
      "\\lambda-rho:0.00000010, Iteration:5100, J:0.00590, |\\Delta J /J|:0.00015167, sparsity:99.85\n",
      "\\lambda-rho:0.00000010, Iteration:5200, J:0.00581, |\\Delta J /J|:0.00014827, sparsity:99.85\n",
      "\\lambda-rho:0.00000010, Iteration:5300, J:0.00573, |\\Delta J /J|:0.00014499, sparsity:99.85\n",
      "\\lambda-rho:0.00000010, Iteration:5400, J:0.00565, |\\Delta J /J|:0.00014178, sparsity:99.84\n",
      "\\lambda-rho:0.00000010, Iteration:5500, J:0.00557, |\\Delta J /J|:0.00013868, sparsity:99.84\n",
      "\\lambda-rho:0.00000010, Iteration:5600, J:0.00549, |\\Delta J /J|:0.00013565, sparsity:99.84\n",
      "\\lambda-rho:0.00000010, Iteration:5700, J:0.00542, |\\Delta J /J|:0.00013271, sparsity:99.84\n",
      "\\lambda-rho:0.00000010, Iteration:5800, J:0.00535, |\\Delta J /J|:0.00012986, sparsity:99.83\n",
      "\\lambda-rho:0.00000010, Iteration:5900, J:0.00528, |\\Delta J /J|:0.00012708, sparsity:99.83\n",
      "\\lambda-rho:0.00000010, Iteration:6000, J:0.00521, |\\Delta J /J|:0.00012438, sparsity:99.83\n",
      "\\lambda-rho:0.00000010, Iteration:6100, J:0.00515, |\\Delta J /J|:0.00012175, sparsity:99.83\n",
      "\\lambda-rho:0.00000010, Iteration:6200, J:0.00509, |\\Delta J /J|:0.00011919, sparsity:99.83\n",
      "\\lambda-rho:0.00000010, Iteration:6300, J:0.00503, |\\Delta J /J|:0.00011669, sparsity:99.83\n",
      "\\lambda-rho:0.00000010, Iteration:6400, J:0.00497, |\\Delta J /J|:0.00011426, sparsity:99.83\n",
      "\\lambda-rho:0.00000010, Iteration:6500, J:0.00492, |\\Delta J /J|:0.00011190, sparsity:99.83\n",
      "\\lambda-rho:0.00000010, Iteration:6600, J:0.00486, |\\Delta J /J|:0.00010959, sparsity:99.82\n",
      "\\lambda-rho:0.00000010, Iteration:6700, J:0.00481, |\\Delta J /J|:0.00010733, sparsity:99.82\n",
      "\\lambda-rho:0.00000010, Iteration:6800, J:0.00476, |\\Delta J /J|:0.00010514, sparsity:99.82\n",
      "\\lambda-rho:0.00000010, Iteration:6900, J:0.00471, |\\Delta J /J|:0.00010300, sparsity:99.82\n",
      "\\lambda-rho:0.00000010, Iteration:7000, J:0.00466, |\\Delta J /J|:0.00010092, sparsity:99.81\n",
      "\\lambda-rho:0.00000010, Iteration:7045, J:0.00464, |\\Delta J /J|:0.00010000, sparsity:99.81\n",
      "  0%|    | 1/500 [07:14<16:24:36, 118.39s/trial, best loss: 0.09538302289975643]2022-10-26 07:57:02.594437: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "MSE:0.00274, R^2:0.98028, MSE-val:0.15956, R^2-val:-0.09248, sparsity:99.81     \n",
      "Params testing:                                                                 \n",
      "{'K': 1, 'activation': 'relu', 'batch_size': 128, 'dropout': 4.641588833612772e-05, 'epochs': 10000, 'kernel_regularizer': 1.1288378916846883e-06, 'knowledge_graph_mask_rho': None, 'knowledge_graph_mask_sparsity_rho': 150, 'lambda_rho': 2.6366508987303607e-07, 'latent_units': 8, 'lr': 0.0001, 'n_steps': 85, 'num_layers': 1, 'optimizer': 'adam', 'output_activation': 'linear', 'post_learning_rate_rho': 1.0, 'regularizer_rho': 'Lasso', 'units': 8, 'use_bias': False}\n",
      "  0%|    | 2/500 [07:16<32:39:03, 236.03s/trial, best loss: 0.09538302289975643]2022-10-26 07:57:05.614486: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_16\n",
      "2022-10-26 07:57:16.557826: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "Batch 3: Invalid loss, terminating training                                     \n",
      "\\lambda-rho:0.00000026, Iteration:0, J:0.03836, |\\Delta J /J|:inf, sparsity:99.90\n",
      "\\lambda-rho:0.00000026, Iteration:100, J:0.03120, |\\Delta J /J|:0.00152607, sparsity:99.90\n",
      "\\lambda-rho:0.00000026, Iteration:200, J:0.02725, |\\Delta J /J|:0.00121865, sparsity:99.90\n",
      "\\lambda-rho:0.00000026, Iteration:300, J:0.02437, |\\Delta J /J|:0.00103248, sparsity:99.89\n",
      "\\lambda-rho:0.00000026, Iteration:400, J:0.02214, |\\Delta J /J|:0.00089914, sparsity:99.89\n",
      "\\lambda-rho:0.00000026, Iteration:500, J:0.02034, |\\Delta J /J|:0.00079637, sparsity:99.89\n",
      "\\lambda-rho:0.00000026, Iteration:600, J:0.01887, |\\Delta J /J|:0.00071379, sparsity:99.88\n",
      "\\lambda-rho:0.00000026, Iteration:700, J:0.01763, |\\Delta J /J|:0.00064545, sparsity:99.88\n",
      "\\lambda-rho:0.00000026, Iteration:800, J:0.01658, |\\Delta J /J|:0.00058770, sparsity:99.87\n",
      "\\lambda-rho:0.00000026, Iteration:900, J:0.01567, |\\Delta J /J|:0.00053818, sparsity:99.87\n",
      "\\lambda-rho:0.00000026, Iteration:1000, J:0.01488, |\\Delta J /J|:0.00049510, sparsity:99.86\n",
      "\\lambda-rho:0.00000026, Iteration:1100, J:0.01419, |\\Delta J /J|:0.00045733, sparsity:99.86\n",
      "\\lambda-rho:0.00000026, Iteration:1200, J:0.01358, |\\Delta J /J|:0.00042393, sparsity:99.85\n",
      "\\lambda-rho:0.00000026, Iteration:1300, J:0.01304, |\\Delta J /J|:0.00039419, sparsity:99.85\n",
      "\\lambda-rho:0.00000026, Iteration:1400, J:0.01255, |\\Delta J /J|:0.00036746, sparsity:99.84\n",
      "\\lambda-rho:0.00000026, Iteration:1500, J:0.01211, |\\Delta J /J|:0.00034335, sparsity:99.84\n",
      "\\lambda-rho:0.00000026, Iteration:1600, J:0.01172, |\\Delta J /J|:0.00032153, sparsity:99.83\n",
      "\\lambda-rho:0.00000026, Iteration:1700, J:0.01136, |\\Delta J /J|:0.00030167, sparsity:99.82\n",
      "\\lambda-rho:0.00000026, Iteration:1800, J:0.01103, |\\Delta J /J|:0.00028355, sparsity:99.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\lambda-rho:0.00000026, Iteration:1900, J:0.01073, |\\Delta J /J|:0.00026695, sparsity:99.80\n",
      "\\lambda-rho:0.00000026, Iteration:2000, J:0.01046, |\\Delta J /J|:0.00025171, sparsity:99.79\n",
      "\\lambda-rho:0.00000026, Iteration:2100, J:0.01021, |\\Delta J /J|:0.00023767, sparsity:99.79\n",
      "\\lambda-rho:0.00000026, Iteration:2200, J:0.00997, |\\Delta J /J|:0.00022469, sparsity:99.78\n",
      "\\lambda-rho:0.00000026, Iteration:2300, J:0.00976, |\\Delta J /J|:0.00021268, sparsity:99.77\n",
      "\\lambda-rho:0.00000026, Iteration:2400, J:0.00956, |\\Delta J /J|:0.00020153, sparsity:99.76\n",
      "\\lambda-rho:0.00000026, Iteration:2500, J:0.00937, |\\Delta J /J|:0.00019116, sparsity:99.75\n",
      "\\lambda-rho:0.00000026, Iteration:2600, J:0.00920, |\\Delta J /J|:0.00018150, sparsity:99.74\n",
      "\\lambda-rho:0.00000026, Iteration:2700, J:0.00904, |\\Delta J /J|:0.00017248, sparsity:99.73\n",
      "\\lambda-rho:0.00000026, Iteration:2800, J:0.00889, |\\Delta J /J|:0.00016406, sparsity:99.72\n",
      "\\lambda-rho:0.00000026, Iteration:2900, J:0.00875, |\\Delta J /J|:0.00015619, sparsity:99.71\n",
      "\\lambda-rho:0.00000026, Iteration:3000, J:0.00861, |\\Delta J /J|:0.00014882, sparsity:99.70\n",
      "\\lambda-rho:0.00000026, Iteration:3100, J:0.00849, |\\Delta J /J|:0.00014191, sparsity:99.68\n",
      "\\lambda-rho:0.00000026, Iteration:3200, J:0.00837, |\\Delta J /J|:0.00013543, sparsity:99.67\n",
      "\\lambda-rho:0.00000026, Iteration:3300, J:0.00826, |\\Delta J /J|:0.00012934, sparsity:99.66\n",
      "\\lambda-rho:0.00000026, Iteration:3400, J:0.00816, |\\Delta J /J|:0.00012359, sparsity:99.65\n",
      "\\lambda-rho:0.00000026, Iteration:3500, J:0.00806, |\\Delta J /J|:0.00011817, sparsity:99.64\n",
      "\\lambda-rho:0.00000026, Iteration:3600, J:0.00797, |\\Delta J /J|:0.00011306, sparsity:99.63\n",
      "\\lambda-rho:0.00000026, Iteration:3700, J:0.00788, |\\Delta J /J|:0.00010823, sparsity:99.62\n",
      "\\lambda-rho:0.00000026, Iteration:3800, J:0.00780, |\\Delta J /J|:0.00010368, sparsity:99.60\n",
      "\\lambda-rho:0.00000026, Iteration:3886, J:0.00773, |\\Delta J /J|:0.00009997, sparsity:99.59\n",
      "  0%|    | 2/500 [10:53<32:39:03, 236.03s/trial, best loss: 0.09538302289975643]2022-10-26 08:00:41.145596: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "MSE:0.00304, R^2:0.97852, MSE-val:0.29165, R^2-val:-1.01153, sparsity:99.59     \n",
      "Params testing:                                                                 \n",
      "{'K': 5, 'activation': 'relu', 'batch_size': 256, 'dropout': 2.1544346900318823e-05, 'epochs': 10000, 'kernel_regularizer': 4.281332398719396e-07, 'knowledge_graph_mask_rho': None, 'knowledge_graph_mask_sparsity_rho': 250, 'lambda_rho': 0.0006158482110660267, 'latent_units': 1, 'lr': 0.00615848211066026, 'n_steps': 80, 'num_layers': 2, 'optimizer': 'adam', 'output_activation': 'linear', 'post_learning_rate_rho': 1.0, 'regularizer_rho': 'Lasso', 'units': 8, 'use_bias': False}\n",
      "  1%|    | 3/500 [10:55<31:29:12, 228.07s/trial, best loss: 0.09538302289975643]2022-10-26 08:00:45.163981: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_16\n",
      "2022-10-26 08:00:46.201316: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "\\lambda-rho:0.00061585, Iteration:0, J:0.97111, |\\Delta J /J|:inf, sparsity:69.45\n",
      "\\lambda-rho:0.00061585, Iteration:18, J:0.13163, |\\Delta J /J|:0.00009073, sparsity:0.02\n",
      "  1%|    | 3/500 [11:03<31:29:12, 228.07s/trial, best loss: 0.09538302289975643]2022-10-26 08:00:52.160384: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "MSE:0.13139, R^2:0.15006, MSE-val:0.12712, R^2-val:0.14395, sparsity:0.02       \n",
      "Params testing:                                                                 \n",
      "{'K': 5, 'activation': 'relu', 'batch_size': 32, 'dropout': 4.641588833612773e-06, 'epochs': 10000, 'kernel_regularizer': 6.951927961775605e-07, 'knowledge_graph_mask_rho': None, 'knowledge_graph_mask_sparsity_rho': 150, 'lambda_rho': 3.359818286283781e-05, 'latent_units': 1, 'lr': 0.00042813323987193956, 'n_steps': 75, 'num_layers': 1, 'optimizer': 'adam', 'output_activation': 'linear', 'post_learning_rate_rho': 1.0, 'regularizer_rho': 'Lasso', 'units': 16, 'use_bias': True}\n",
      "  1%|    | 4/500 [11:06<19:38:37, 142.58s/trial, best loss: 0.09538302289975643]2022-10-26 08:00:56.429912: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_16\n",
      "2022-10-26 08:00:57.040993: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "\\lambda-rho:0.00003360, Iteration:0, J:0.10087, |\\Delta J /J|:inf, sparsity:84.10\n",
      "\\lambda-rho:0.00003360, Iteration:22, J:0.09703, |\\Delta J /J|:0.00008910, sparsity:22.19\n",
      "  1%|    | 4/500 [11:41<19:38:37, 142.58s/trial, best loss: 0.09538302289975643]2022-10-26 08:01:29.553583: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "MSE:0.08798, R^2:0.41505, MSE-val:0.08967, R^2-val:0.38018, sparsity:22.19      \n",
      "Params testing:                                                                 \n",
      "{'K': 1, 'activation': 'relu', 'batch_size': 32, 'dropout': 4.641588833612772e-05, 'epochs': 10000, 'kernel_regularizer': 0.0006158482110660261, 'knowledge_graph_mask_rho': None, 'knowledge_graph_mask_sparsity_rho': 100, 'lambda_rho': 0.00023357214690901214, 'latent_units': 16, 'lr': 0.004832930238571752, 'n_steps': 60, 'num_layers': 1, 'optimizer': 'adam', 'output_activation': 'linear', 'post_learning_rate_rho': 1.0, 'regularizer_rho': 'Lasso', 'units': 8, 'use_bias': False}\n",
      "  1%|    | 5/500 [11:43<14:21:39, 104.44s/trial, best loss: 0.08967379171573862]2022-10-26 08:01:32.443484: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_16\n",
      "2022-10-26 08:01:32.949214: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "\\lambda-rho:0.00023357, Iteration:0, J:0.27001, |\\Delta J /J|:inf, sparsity:76.44\n",
      "\\lambda-rho:0.00023357, Iteration:19, J:0.09888, |\\Delta J /J|:0.00007435, sparsity:0.02\n",
      "  1%|    | 5/500 [12:09<14:21:39, 104.44s/trial, best loss: 0.08967379171573862]2022-10-26 08:01:57.414126: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "MSE:0.09885, R^2:0.34147, MSE-val:0.09564, R^2-val:0.33369, sparsity:0.02       \n",
      "Params testing:                                                                 \n",
      "{'K': 3, 'activation': 'relu', 'batch_size': 128, 'dropout': 1e-06, 'epochs': 10000, 'kernel_regularizer': 7.847599703514607e-06, 'knowledge_graph_mask_rho': None, 'knowledge_graph_mask_sparsity_rho': 150, 'lambda_rho': 8.858667904100833e-05, 'latent_units': 8, 'lr': 0.01, 'n_steps': 70, 'num_layers': 1, 'optimizer': 'adam', 'output_activation': 'linear', 'post_learning_rate_rho': 1.0, 'regularizer_rho': 'Lasso', 'units': 16, 'use_bias': True}\n",
      "  1%|     | 6/500 [12:11<10:45:27, 78.39s/trial, best loss: 0.08967379171573862]2022-10-26 08:02:00.561939: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_16\n",
      "2022-10-26 08:02:01.146056: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "\\lambda-rho:0.00008859, Iteration:0, J:3.56337, |\\Delta J /J|:inf, sparsity:99.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\lambda-rho:0.00008859, Iteration:1, J:200.70283, |\\Delta J /J|:0.98224553, sparsity:99.14\n",
      "WARNING!!!! Objective increased!!! Restarting from initial solution with learning rate 0.5. \n",
      "\\lambda-rho:0.00008859, Iteration:3, J:45.62361, |\\Delta J /J|:0.92189632, sparsity:99.56\n",
      "WARNING!!!! Objective increased!!! Restarting from initial solution with learning rate 0.25. \n",
      "\\lambda-rho:0.00008859, Iteration:5, J:10.46353, |\\Delta J /J|:0.65944848, sparsity:99.73\n",
      "WARNING!!!! Objective increased!!! Restarting from initial solution with learning rate 0.125. \n",
      "\\lambda-rho:0.00008859, Iteration:8, J:5.71672, |\\Delta J /J|:0.39134811, sparsity:99.80\n",
      "WARNING!!!! Objective increased!!! Restarting from initial solution with learning rate 0.0625. \n",
      "\\lambda-rho:0.00008859, Iteration:100, J:1.19706, |\\Delta J /J|:0.00402138, sparsity:98.66\n",
      "\\lambda-rho:0.00008859, Iteration:200, J:0.89490, |\\Delta J /J|:0.00202942, sparsity:96.30\n",
      "\\lambda-rho:0.00008859, Iteration:300, J:0.77682, |\\Delta J /J|:0.00080340, sparsity:92.67\n",
      "\\lambda-rho:0.00008859, Iteration:368, J:0.75200, |\\Delta J /J|:0.00008603, sparsity:89.31\n",
      "  1%|     | 6/500 [12:32<10:45:27, 78.39s/trial, best loss: 0.08967379171573862]2022-10-26 08:02:20.439438: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "MSE:0.34347, R^2:-2.05586, MSE-val:0.39349, R^2-val:-1.69073, sparsity:89.31    \n",
      "Params testing:                                                                 \n",
      "{'K': 4, 'activation': 'relu', 'batch_size': 64, 'dropout': 1e-06, 'epochs': 10000, 'kernel_regularizer': 8.858667904100833e-05, 'knowledge_graph_mask_rho': None, 'knowledge_graph_mask_sparsity_rho': 200, 'lambda_rho': 0.00023357214690901214, 'latent_units': 16, 'lr': 0.0008858667904100823, 'n_steps': 40, 'num_layers': 2, 'optimizer': 'adam', 'output_activation': 'linear', 'post_learning_rate_rho': 1.0, 'regularizer_rho': 'Lasso', 'units': 32, 'use_bias': True}\n",
      "  1%|      | 7/500 [12:34<8:15:49, 60.34s/trial, best loss: 0.08967379171573862]2022-10-26 08:02:24.569986: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_16\n",
      "2022-10-26 08:02:25.609591: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "^C\n",
      "  1%|    | 7/500 [12:54<15:08:50, 110.61s/trial, best loss: 0.08967379171573862]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/shibal/GREGNETS/src/NNPC/NGCNPC-Tuning.py\", line 320, in <module>\n",
      "    return_argmin=False)\n",
      "  File \"/home/gridsan/shibal/.conda/envs/GraphNetworks/lib/python3.7/site-packages/hyperopt/fmin.py\", line 522, in fmin\n",
      "    trials_save_file=trials_save_file,\n",
      "  File \"/home/gridsan/shibal/.conda/envs/GraphNetworks/lib/python3.7/site-packages/hyperopt/base.py\", line 699, in fmin\n",
      "    trials_save_file=trials_save_file,\n",
      "  File \"/home/gridsan/shibal/.conda/envs/GraphNetworks/lib/python3.7/site-packages/hyperopt/fmin.py\", line 553, in fmin\n",
      "    rval.exhaust()\n",
      "  File \"/home/gridsan/shibal/.conda/envs/GraphNetworks/lib/python3.7/site-packages/hyperopt/fmin.py\", line 356, in exhaust\n",
      "    self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n",
      "  File \"/home/gridsan/shibal/.conda/envs/GraphNetworks/lib/python3.7/site-packages/hyperopt/fmin.py\", line 292, in run\n",
      "    self.serial_evaluate()\n",
      "  File \"/home/gridsan/shibal/.conda/envs/GraphNetworks/lib/python3.7/site-packages/hyperopt/fmin.py\", line 170, in serial_evaluate\n",
      "    result = self.domain.evaluate(spec, ctrl)\n",
      "  File \"/home/gridsan/shibal/.conda/envs/GraphNetworks/lib/python3.7/site-packages/hyperopt/base.py\", line 907, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/home/gridsan/shibal/GREGNETS/src/NNPC/NGCNPC-Tuning.py\", line 273, in f_model\n",
      "    verbose=0, \n",
      "  File \"/home/gridsan/shibal/.conda/envs/GraphNetworks/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\n",
      "    tmp_logs = self.train_function(iterator)\n",
      "  File \"/home/gridsan/shibal/.conda/envs/GraphNetworks/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/gridsan/shibal/.conda/envs/GraphNetworks/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\", line 855, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/gridsan/shibal/.conda/envs/GraphNetworks/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 2943, in __call__\n",
      "    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "  File \"/home/gridsan/shibal/.conda/envs/GraphNetworks/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\n",
      "  File \"/home/gridsan/shibal/.conda/envs/GraphNetworks/lib/python3.7/site-packages/tensorflow/python/eager/function.py\", line 560, in call\n",
      "    ctx=ctx)\n",
      "  File \"/home/gridsan/shibal/.conda/envs/GraphNetworks/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
      "    inputs, attrs, num_outputs)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b804e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-GraphNetworks] *",
   "language": "python",
   "name": "conda-env-.conda-GraphNetworks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
