{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c5da5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"\"\"\n",
    "/home/gridsan/shibal/.conda/envs/GraphNetworks/bin/python -u /home/gridsan/shibal/GREGNETS/src/VARPC/VARPC_tuning.py \\\n",
    "--load_directory /home/gridsan/shibal/FinancialForecasting_shared/data/ --cohort 'SP1500' --time_series 'volatilities' \\\n",
    "--regularizer 'Lasso' --KG_mask 'soft' --mask_sparsity 100  \\\n",
    "--num_training_years 2 --n_steps 1 \\\n",
    "--version 2 |& tee -a '/home/gridsan/shibal/GREGNETS/src/VARPC/logs-SP1500/MODELVARPC_COHORTSP1500_TSvolume_REGLasso_KGcosearch_MASKsoft_LEVEL100_Y2_NSTEPS1_V2/output.txt'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0fbea3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tee: /home/gridsan/shibal/GREGNETS/src/VARPC/logs-SP1500/MODELVARPC_COHORTSP1500_TSvolume_REGLasso_KGcosearch_MASKsoft_LEVEL100_Y2_NSTEPS1_V2/output.txt: No such file or directory\n",
      "3.7.10 (default, Feb 26 2021, 18:47:35) \n",
      "[GCC 7.3.0] linux /home/gridsan/shibal/.conda/envs/GraphNetworks/bin/python\n",
      "/home/gridsan/shibal/FinancialForecasting_shared/data/\n",
      "volatilities/SP1500/yahoo_companies_volatilities_residuals_2Y.csv\n",
      "(N, T, p): (503, 1072, 1)\n",
      "Training Data Shapes: (503, 1072, 1) , (503, 1072)\n",
      "Validation Data Shapes: (252, 1072, 1) , (252, 1072)\n",
      "Test Data Shapes: (252, 1072, 1) , (252, 1072)\n",
      "Starting optimization for pre-estimator for A...\n",
      "Iter    objective   relative_objective\n",
      "    0    0.116599    inf\n",
      "  100    0.088848    0.000333\n",
      "  200    0.087811    0.000026\n",
      "Solution converged in 240 iterations with final objective=0.08775 and relative objective=0.00000978<=1e-05\n",
      "/home/gridsan/shibal/GREGNETS/src/VARPC/pre_estimators.py:285: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  M[i,j] = cmax/co[i,j]\n",
      "/home/gridsan/shibal/GREGNETS/src/VARPC/pre_estimators.py:286: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  M[j,i] = cmax/co[i,j]\n",
      "lambda_A:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "lambda_rho:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "Warm start either not set or it is yet to warm up.\n",
      "Initializing using arguments from init or as a default zero matrix\n",
      "Starting optimization for lambda_A: 0.0001 lambda_rho: 0.0001\n",
      "Iter    objective   convergence_count   no_progress_count\n",
      "    0    0.174357773    0   0\n",
      "   10    0.172149855    0   0\n",
      "   20    0.170987465    0   0\n",
      "   30    0.170279613    0   0\n",
      "   40    0.169792619    0   0\n",
      "   50    0.169422195    0   0\n",
      "   60    0.169121057    0   0\n",
      "   70    0.168867181    0   0\n",
      "   80    0.168645726    0   0\n",
      "   90    0.168449876    0   0\n",
      "  100    0.168275158    0   0\n",
      "  110    0.168117286    0   0\n",
      "  120    0.167973317    0   0\n",
      "  130    0.167841117    0   0\n",
      "  140    0.167720112    0   0\n",
      "  150    0.167609320    0   0\n",
      "  160    0.167508419    0   0\n",
      "  170    0.167415314    0   0\n",
      "  180    0.167328813    0   0\n",
      "  190    0.167248046    0   0\n",
      "  200    0.167172753    0   0\n",
      "  210    0.167102265    0   0\n",
      "  220    0.167036293    0   0\n",
      "  230    0.166974759    0   0\n",
      "  240    0.166917087    0   0\n",
      "  250    0.166863095    0   0\n",
      "  260    0.166812225    0   0\n",
      "  270    0.166764125    0   0\n",
      "  280    0.166718507    0   0\n",
      "  290    0.166675210    0   0\n",
      "  300    0.166634489    0   0\n",
      "  310    0.166596014    0   0\n",
      "  320    0.166559457    0   0\n",
      "  330    0.166524777    0   0\n",
      "  340    0.166491933    0   0\n",
      "  350    0.166460882    0   0\n",
      "  360    0.166431434    0   0\n",
      "  370    0.166403264    0   0\n",
      "  380    0.166376447    0   0\n",
      "  390    0.166351072    0   0\n",
      "  400    0.166326793    0   0\n",
      "  410    0.166303577    0   0\n",
      "  420    0.166281345    0   0\n",
      "  430    0.166260134    0   0\n",
      "  440    0.166240067    0   0\n",
      "  450    0.166221095    0   0\n",
      "  460    0.166203111    0   0\n",
      "  470    0.166185899    0   0\n",
      "  480    0.166169377    0   0\n",
      "  490    0.166153569    0   0\n",
      "  500    0.166138436    0   0\n",
      "  510    0.166123936    0   0\n",
      "  520    0.166110064    0   0\n",
      "  530    0.166096780    0   0\n",
      "  540    0.166083977    0   0\n",
      "  550    0.166071627    0   0\n",
      "  560    0.166059708    0   0\n",
      "  570    0.166048233    0   0\n",
      "  580    0.166037167    0   0\n",
      "  590    0.166026559    0   0\n",
      "  600    0.166016317    0   0\n",
      "  610    0.166006420    8   0\n",
      "Solution converged in 612 iterations with final\n",
      "                                            objective=0.166004479, slow progress with global relative\n",
      "                                            objective=0.000000969<=1e-06 for 10 non-consecutive\n",
      "                                            iterations\n",
      "  612    0.166004479    10   0\n",
      "0.000100000000  0.000100000000  0.15481  0.15278  0.06306  0.05\n",
      "Starting optimization for lambda_A: 0.0001 lambda_rho: 7.498942093324559e-05\n",
      "Iter    objective   convergence_count   no_progress_count\n",
      "    0    0.163203178    0   0\n",
      "   10    0.163075618    0   0\n",
      "   20    0.162970248    0   0\n",
      "   30    0.162880713    0   0\n",
      "   40    0.162803205    0   0\n",
      "   50    0.162735271    0   0\n",
      "   60    0.162675161    0   0\n",
      "   70    0.162621562    0   0\n",
      "   80    0.162573477    0   0\n",
      "   90    0.162530090    0   0\n",
      "  100    0.162490724    0   0\n",
      "  110    0.162454808    0   0\n",
      "  120    0.162421884    0   0\n",
      "  130    0.162391617    0   0\n",
      "  140    0.162363719    0   0\n",
      "  150    0.162337951    0   0\n",
      "  160    0.162314014    0   0\n",
      "  170    0.162291695    0   0\n",
      "  180    0.162270884    0   0\n",
      "  190    0.162251436    0   0\n",
      "  200    0.162233209    0   0\n",
      "  210    0.162216088    0   0\n",
      "  220    0.162199965    0   0\n",
      "  230    0.162184730    0   0\n",
      "  240    0.162170324    0   0\n",
      "  250    0.162156692    0   0\n",
      "  260    0.162143812    0   0\n",
      "  270    0.162131616    0   0\n",
      "  280    0.162120052    0   0\n",
      "  290    0.162109123    0   0\n",
      "  300    0.162098719    0   0\n",
      "  310    0.162088796    7   0\n",
      "Solution converged in 313 iterations with final\n",
      "                                            objective=0.162085909, slow progress with global relative\n",
      "                                            objective=0.000000958<=1e-06 for 10 non-consecutive\n",
      "                                            iterations\n",
      "  313    0.162085909    10   0\n",
      "0.000100000000  0.000074989421  0.14876  0.14799  0.08496  0.08\n",
      "Starting optimization for lambda_A: 0.0001 lambda_rho: 5.623413251903491e-05\n",
      "Iter    objective   convergence_count   no_progress_count\n",
      "    0    0.158751188    0   0\n",
      "   10    0.158602050    0   0\n",
      "   20    0.158479174    0   0\n",
      "   30    0.158375081    0   0\n",
      "   40    0.158285203    0   0\n",
      "   50    0.158206544    0   0\n",
      "   60    0.158136983    0   0\n",
      "   70    0.158074930    0   0\n",
      "   80    0.158019143    0   0\n",
      "   90    0.157968718    0   0\n",
      "  100    0.157922838    0   0\n",
      "  110    0.157880894    0   0\n",
      "  120    0.157842369    0   0\n",
      "  130    0.157806889    0   0\n",
      "  140    0.157774062    0   0\n",
      "  150    0.157743647    0   0\n",
      "  160    0.157715318    0   0\n",
      "  170    0.157688880    0   0\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d159b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "command = \"\"\"\n",
    "/home/gridsan/shibal/.conda/envs/GraphNetworks/bin/python -u /home/gridsan/shibal/GREGNETS/src/NNPC/NGCNPC-Tuning.py \\\n",
    "--load_directory '/home/gridsan/shibal/FinancialForecasting_shared/data/' --time_series 'volatilities' --cohort 'SP1500' \\\n",
    "--num_training_years 2 \\\n",
    "--no-mask \\\n",
    "--ntrials 2 --version 1 \\\n",
    "|& tee -a '/home/gridsan/shibal/GREGNETS/src/NNPC/logs-SP1500/MODELNGCNPC_COHORTSP1500_TSvolatilities_REGLasso_KGcosearch_MASKFalse_Y2_V1/output.txt'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "015c4843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tee: /home/gridsan/shibal/GREGNETS/src/NNPC/logs-SP1500/MODELNGCNPC_COHORTSP1500_TSvolatilities_REGLasso_KGcosearch_MASKFalse_Y2_V1/output.txt: No such file or directory\n",
      "3.7.10 (default, Feb 26 2021, 18:47:35) \n",
      "[GCC 7.3.0] linux /home/gridsan/shibal/.conda/envs/GraphNetworks/bin/python\n",
      "2022-10-26 08:05:50.218018: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-10-26 08:05:53.189257: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-10-26 08:05:53.189461: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /state/partition1/llgrid/pkg/anaconda/anaconda3-2021a/pkgs/cudatoolkit-10.2.89-hfd86e86_1/lib:/state/partition1/llgrid/pkg/anaconda/anaconda3-2021a/lib\n",
      "2022-10-26 08:05:53.189473: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-26 08:05:53.189495: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (d-6-4-2): /proc/driver/nvidia/version does not exist\n",
      "2022-10-26 08:05:53.189776: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-26 08:05:53.200958: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "/home/gridsan/shibal/GREGNETS/src/data_utils.py:337: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  start_date=pd.datetime(2005,1,1),\n",
      "/home/gridsan/shibal/GREGNETS/src/data_utils.py:338: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  end_date=pd.datetime(2020,12,31)\n",
      "/home/gridsan/shibal/GREGNETS/src/data_utils.py:895: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  start_date=pd.datetime(2005,1,1),\n",
      "/home/gridsan/shibal/GREGNETS/src/data_utils.py:896: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  end_date=pd.datetime(2020,12,31)):\n",
      "/home/gridsan/shibal/FinancialForecasting_shared/data/\n",
      "volatilities/SP1500/yahoo_companies_volatilities_residuals_2Y.csv\n",
      "(440, 64, 1072) (440, 1072) (252, 64, 1072) (252, 1072) (252, 64, 1072) (252, 1072)\n",
      "(440, 1072, 64) (440, 1072) (252, 1072, 64) (252, 1072) (252, 1072, 64) (252, 1072)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method NGraphConvolution.call of <src.NNPC.Layers.layers.NGraphConvolution object at 0x7f1bd8683a90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 1072, 64)]        0         \n",
      "_________________________________________________________________\n",
      "n_graph_convolution (NGraphC (None, 1072, 1)           2113      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1072, 1)           0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 1072)              0         \n",
      "=================================================================\n",
      "Total params: 2,113\n",
      "Trainable params: 2,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2022-10-26 08:05:54.457218: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-10-26 08:05:54.461737: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2400000000 Hz\n",
      "Epoch 1/10000\n",
      "7/7 [==============================] - 2s 140ms/step - loss: 0.3520 - mse: 0.3520 - val_loss: 0.2864 - val_mse: 0.2864\n",
      "Epoch 2/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2822 - mse: 0.2822 - val_loss: 0.2306 - val_mse: 0.2306\n",
      "Epoch 3/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2313 - mse: 0.2313 - val_loss: 0.1956 - val_mse: 0.1956\n",
      "Epoch 4/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1970 - mse: 0.1970 - val_loss: 0.1696 - val_mse: 0.1696\n",
      "Epoch 5/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1725 - mse: 0.1725 - val_loss: 0.1527 - val_mse: 0.1527\n",
      "Epoch 6/10000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1563 - mse: 0.1563 - val_loss: 0.1408 - val_mse: 0.1408\n",
      "Epoch 7/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1427 - mse: 0.1427 - val_loss: 0.1314 - val_mse: 0.1314\n",
      "Epoch 8/10000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1329 - mse: 0.1329 - val_loss: 0.1239 - val_mse: 0.1239\n",
      "Epoch 9/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1252 - mse: 0.1252 - val_loss: 0.1181 - val_mse: 0.1181\n",
      "Epoch 10/10000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1196 - mse: 0.1196 - val_loss: 0.1137 - val_mse: 0.1137\n",
      "Epoch 11/10000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1165 - mse: 0.1165 - val_loss: 0.1103 - val_mse: 0.1103\n",
      "Epoch 12/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1117 - mse: 0.1117 - val_loss: 0.1077 - val_mse: 0.1077\n",
      "Epoch 13/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1096 - mse: 0.1096 - val_loss: 0.1058 - val_mse: 0.1058\n",
      "Epoch 14/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1084 - mse: 0.1084 - val_loss: 0.1042 - val_mse: 0.1042\n",
      "Epoch 15/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1064 - mse: 0.1064 - val_loss: 0.1030 - val_mse: 0.1030\n",
      "Epoch 16/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1057 - mse: 0.1057 - val_loss: 0.1021 - val_mse: 0.1021\n",
      "Epoch 17/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1055 - mse: 0.1055 - val_loss: 0.1013 - val_mse: 0.1013\n",
      "Epoch 18/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1039 - mse: 0.1039 - val_loss: 0.1007 - val_mse: 0.1007\n",
      "Epoch 19/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1031 - mse: 0.1031 - val_loss: 0.1002 - val_mse: 0.1002\n",
      "Epoch 20/10000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1026 - mse: 0.1026 - val_loss: 0.0998 - val_mse: 0.0998\n",
      "Epoch 21/10000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1022 - mse: 0.1022 - val_loss: 0.0994 - val_mse: 0.0994\n",
      "Epoch 22/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1036 - mse: 0.1036 - val_loss: 0.0990 - val_mse: 0.0990\n",
      "Epoch 23/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1019 - mse: 0.1019 - val_loss: 0.0988 - val_mse: 0.0988\n",
      "Epoch 24/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1016 - mse: 0.1016 - val_loss: 0.0985 - val_mse: 0.0985\n",
      "Epoch 25/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1009 - mse: 0.1009 - val_loss: 0.0983 - val_mse: 0.0983\n",
      "Epoch 26/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1008 - mse: 0.1008 - val_loss: 0.0981 - val_mse: 0.0981\n",
      "Epoch 27/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1012 - mse: 0.1012 - val_loss: 0.0980 - val_mse: 0.0980\n",
      "Epoch 28/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1015 - mse: 0.1015 - val_loss: 0.0978 - val_mse: 0.0978\n",
      "Epoch 29/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1002 - mse: 0.1002 - val_loss: 0.0976 - val_mse: 0.0976\n",
      "Epoch 30/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1012 - mse: 0.1012 - val_loss: 0.0975 - val_mse: 0.0975\n",
      "Epoch 31/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1009 - mse: 0.1009 - val_loss: 0.0974 - val_mse: 0.0974\n",
      "Epoch 32/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1000 - mse: 0.1000 - val_loss: 0.0973 - val_mse: 0.0973\n",
      "Epoch 33/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0994 - mse: 0.0994 - val_loss: 0.0972 - val_mse: 0.0972\n",
      "Epoch 34/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0998 - mse: 0.0998 - val_loss: 0.0970 - val_mse: 0.0970\n",
      "Epoch 35/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0997 - mse: 0.0997 - val_loss: 0.0970 - val_mse: 0.0970\n",
      "Epoch 36/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0990 - mse: 0.0990 - val_loss: 0.0969 - val_mse: 0.0969\n",
      "Epoch 37/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.0968 - val_mse: 0.0968\n",
      "Epoch 38/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1001 - mse: 0.1001 - val_loss: 0.0967 - val_mse: 0.0967\n",
      "Epoch 39/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0990 - mse: 0.0990 - val_loss: 0.0966 - val_mse: 0.0966\n",
      "Epoch 40/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0991 - mse: 0.0991 - val_loss: 0.0966 - val_mse: 0.0966\n",
      "Epoch 41/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.0965 - val_mse: 0.0965\n",
      "Epoch 42/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.1008 - mse: 0.1008 - val_loss: 0.0965 - val_mse: 0.0965\n",
      "Epoch 43/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0964 - val_mse: 0.0964\n",
      "Epoch 44/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1004 - mse: 0.1004 - val_loss: 0.0963 - val_mse: 0.0963\n",
      "Epoch 45/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0995 - mse: 0.0995 - val_loss: 0.0963 - val_mse: 0.0963\n",
      "Epoch 46/10000\n",
      "7/7 [==============================] - 0s 45ms/step - loss: 0.0998 - mse: 0.0998 - val_loss: 0.0963 - val_mse: 0.0963\n",
      "Epoch 47/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0962 - val_mse: 0.0962\n",
      "Epoch 48/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0996 - mse: 0.0996 - val_loss: 0.0961 - val_mse: 0.0961\n",
      "Epoch 49/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0981 - mse: 0.0981 - val_loss: 0.0961 - val_mse: 0.0961\n",
      "Epoch 50/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.0961 - val_mse: 0.0961\n",
      "Epoch 51/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0998 - mse: 0.0998 - val_loss: 0.0961 - val_mse: 0.0961\n",
      "Epoch 52/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.0960 - val_mse: 0.0960\n",
      "Epoch 53/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0960 - val_mse: 0.0960\n",
      "Epoch 54/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.0961 - val_mse: 0.0961\n",
      "Epoch 55/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0959 - val_mse: 0.0959\n",
      "Epoch 56/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0959 - val_mse: 0.0959\n",
      "Epoch 57/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.0959 - val_mse: 0.0959\n",
      "Epoch 58/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0959 - val_mse: 0.0959\n",
      "Epoch 59/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 60/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1009 - mse: 0.1009 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 61/10000\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0959 - val_mse: 0.0959\n",
      "Epoch 62/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 63/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0971 - mse: 0.0971 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 64/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 65/10000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987 - mse: 0.098 - 1s 89ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 66/10000\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 67/10000\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.0994 - mse: 0.0994 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 68/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 69/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 70/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 71/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 72/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 73/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0990 - mse: 0.0990 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 74/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 75/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 76/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 77/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.0958 - val_mse: 0.0958\n",
      "Epoch 78/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 79/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 80/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 81/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0991 - mse: 0.0991 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 82/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.1003 - mse: 0.1003 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 83/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0995 - mse: 0.0995 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 84/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0996 - mse: 0.0996 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 85/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 86/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 87/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 88/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0972 - mse: 0.0972 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 89/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0975 - mse: 0.0975 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 90/10000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0971 - mse: 0.097 - 0s 25ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 91/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 92/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0970 - mse: 0.0970 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 93/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 94/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 95/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 96/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0994 - mse: 0.0994 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 97/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 98/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 99/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 100/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 101/10000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0991 - mse: 0.099 - 0s 25ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 102/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 103/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 104/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 105/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0990 - mse: 0.0990 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 106/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 107/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 108/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 109/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 110/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0980 - mse: 0.0980 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 111/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0972 - mse: 0.0972 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 112/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 113/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 114/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 115/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0990 - mse: 0.0990 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 116/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 117/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0980 - mse: 0.0980 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 118/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 119/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0981 - mse: 0.0981 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 120/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 121/10000\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 122/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 123/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0975 - mse: 0.0975 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 124/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 125/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0989 - mse: 0.0989 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 126/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0975 - mse: 0.0975 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 127/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 128/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0995 - mse: 0.0995 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 129/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 130/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 131/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 132/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 133/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0972 - mse: 0.0972 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 134/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 135/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 136/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 137/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 138/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 139/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0980 - mse: 0.0980 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 140/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 141/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0968 - mse: 0.0968 - val_loss: 0.0957 - val_mse: 0.0957\n",
      "Epoch 142/10000\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0993 - mse: 0.0993 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 143/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 144/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 145/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 146/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0981 - mse: 0.0981 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 147/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0981 - mse: 0.0981 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 148/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0991 - mse: 0.0991 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 149/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 150/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0965 - mse: 0.0965 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 151/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 152/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0999 - mse: 0.0999 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 153/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.0954 - val_mse: 0.0954\n",
      "Epoch 154/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 155/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 156/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 157/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0983 - mse: 0.0983 - val_loss: 0.0954 - val_mse: 0.0954\n",
      "Epoch 158/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 159/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0997 - mse: 0.0997 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 160/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0978 - mse: 0.0978 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 161/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0977 - mse: 0.0977 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 162/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 163/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0966 - mse: 0.0966 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 164/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0979 - mse: 0.0979 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 165/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0986 - mse: 0.0986 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 166/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0982 - mse: 0.0982 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 167/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0988 - mse: 0.0988 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 168/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 169/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0985 - mse: 0.0985 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 170/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0992 - mse: 0.0992 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 171/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0970 - mse: 0.0970 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 172/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 173/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0976 - mse: 0.0976 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 174/10000\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0973 - mse: 0.0973 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 175/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0987 - mse: 0.0987 - val_loss: 0.0956 - val_mse: 0.0956\n",
      "Epoch 176/10000\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Epoch 177/10000\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.0987 - mse: 0.098 - 0s 25ms/step - loss: 0.0984 - mse: 0.0984 - val_loss: 0.0955 - val_mse: 0.0955\n",
      "Params testing:                                      \n",
      "{'K': 1, 'activation': 'relu', 'batch_size': 64, 'dropout': 1e-07, 'epochs': 10000, 'kernel_regularizer': 2.6366508987303554e-07, 'knowledge_graph_mask_rho': None, 'knowledge_graph_mask_sparsity_rho': 100, 'lambda_rho': 0.000379269019073225, 'latent_units': 16, 'lr': 0.01, 'n_steps': 85, 'num_layers': 2, 'optimizer': 'adam', 'output_activation': 'linear', 'post_learning_rate_rho': 1.0, 'regularizer_rho': 'Lasso', 'units': 32, 'use_bias': True}\n",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]WARNING:tensorflow:AutoGraph could not transform <bound method PseudoLikelihood.call of <src.NNPC.Layers.layers.PseudoLikelihood object at 0x7f1bd0103c90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2022-10-26 08:06:28.391057: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_16\n",
      "2022-10-26 08:06:29.213048: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "\\lambda-rho:0.00037927, Iteration:0, J:0.63256, |\\Delta J /J|:inf, sparsity:79.91\n",
      "\\lambda-rho:0.00037927, Iteration:18, J:0.09809, |\\Delta J /J|:0.00009896, sparsity:0.00\n",
      "  0%|          | 0/2 [01:06<?, ?trial/s, best loss=?]2022-10-26 08:07:33.701581: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "MSE:0.09808, R^2:0.34302, MSE-val:0.09606, R^2-val:0.33082, sparsity:0.00\n",
      "Params testing:                                                                 \n",
      "{'K': 3, 'activation': 'relu', 'batch_size': 32, 'dropout': 0.0001, 'epochs': 10000, 'kernel_regularizer': 0.0006158482110660261, 'knowledge_graph_mask_rho': None, 'knowledge_graph_mask_sparsity_rho': 250, 'lambda_rho': 1.8329807108324375e-06, 'latent_units': 8, 'lr': 0.01, 'n_steps': 50, 'num_layers': 1, 'optimizer': 'adam', 'output_activation': 'linear', 'post_learning_rate_rho': 1.0, 'regularizer_rho': 'Lasso', 'units': 8, 'use_bias': True}\n",
      " 50%|█████     | 1/2 [01:09<01:09, 69.04s/trial, best loss: 0.09606299830823214]2022-10-26 08:07:37.108410: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_16\n",
      "2022-10-26 08:07:37.679777: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_1/pseudo_likelihood/cond/branch_executed/_10\n",
      "Batch 7: Invalid loss, terminating training                                     \n",
      "MSE:inf, R^2:-inf, MSE-val:inf, R^2-val:-inf, sparsity:100.00                   \n",
      "100%|██████████| 2/2 [01:13<00:00, 36.78s/trial, best loss: 0.09606299830823214]\n",
      "Training completed in 00:01:13.63\n",
      "(671, 1072, 85) (671, 1072) (252, 1072, 85) (252, 1072)\n",
      "2022-10-26 08:08:11.317247: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_4/pseudo_likelihood_1/cond/branch_executed/_16\n",
      "2022-10-26 08:08:12.130004: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_4/pseudo_likelihood_1/cond/branch_executed/_10\n",
      "\\lambda-rho:0.00037927, Iteration:0, J:0.64657, |\\Delta J /J|:inf, sparsity:80.37\n",
      "\\lambda-rho:0.00037927, Iteration:19, J:0.09714, |\\Delta J /J|:0.00006467, sparsity:0.00\n",
      "2022-10-26 08:10:15.422446: W tensorflow/core/grappler/optimizers/loop_optimizer.cc:906] Skipping loop optimization for Merge node with control input: model_4/pseudo_likelihood_1/cond/branch_executed/_10\n",
      "MSE:0.09713, R^2:0.34958, MSE-test:0.10013, R^2-test:0.35607, sparsity:0.00\n",
      "2022-10-26 08:10:22.880291: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Figure(1300x1000)\n"
     ]
    }
   ],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89113eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-GraphNetworks] *",
   "language": "python",
   "name": "conda-env-.conda-GraphNetworks-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
